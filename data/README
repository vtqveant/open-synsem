http://alt.qcri.org/semeval2014/task6/


***********************************************************************************************************************


Supervised Semantic Parsing of Robotic Spatial Commands

INTRODUCTION

Recent research into semantic parsing has focused on approaches that translate natural language into formal meaning
representations. In non-contextual semantic parsing, the aim is to translate directly into meaning representations such
as logical form. In contrast, SemEval-2014 Task 6 focuses on contextual semantic parsing of robotic spatial commands,
where the additional context of spatial scenes can be used to guide a semantic parser. For example, the command "move
the red block on top of the blue cube on the yellow one" is syntactically ambiguous with three plausible readings.
These can be disambiguated by knowing the colors, shapes and positions of blocks as context for the command. This
additional context provides useful cues for semantically resolving syntactically ambiguous constructions such as verbal
argument and prepositional phrase attachment. The challenge for this task will be to either enhance existing approaches
(such as CCG semantic parsing) and to include spatial context to guide the parsing process, or to adapt novel approaches
for syntactic parsing with semantic disambiguation to the robotic spatial commands domain.

TRAINING RESOURCES

A new annotated resource has been developed as a dataset for comparing and evaluating different approaches for this
task. The Robot Commands Treebank (Dukes, 2013) pairs 3,394 sentences (41,158 words) with a formal Robot Control
Language (RCL), manually annotated within the context of spatial scenes. RCL is a linguistically-oriented formal
language for controlling a robot arm, that represents entities, attributes, anaphora, ellipsis and qualitative spatial
relations. Each scene is a formal description of a discrete 8x8x8 3D game board consisting of colored blocks. Using
crowdsourcing, pairs of before and after scenes have been labelled with commands that provide natural language
instructions to a robot to pick up, move and rearrange blocks on a game board. In the training data, each command has
been translated into RCL. The complete treebank with automatically rendered illustrations can be browsed online:

 http://www.trainrobots.com/treebank.jsp

The following resources are available for SemEval-2014 Task 6:

  * Training data from the Robot Commands Treebank, consisting of sentences paired with RCL.
  * Word-aligned semantic trees that map RCL elements to natural language words. This additional data can be used to
    either adapt a lexicon for this domain, or to improve grounded language acquisition components.
  * A spatial planner, made available as a Java API, which takes as input an RCL fragment, and determines if that
    fragment is compatible with spatial context. For example, providing the RCL input
    (entity: (type: cube) (color: red)) to the planner will return a list of zero or more red blocks together with
    their positions, depending on the state of the game board. The planner can also be used to validate possible
    actions for the robot arm (events in RCL).


TASK DESCRIPTION

During evaluation, systems submitted by task participants will be expected to parse previously unseen sentences for
new board layouts, and generate the correct corresponding RCL statement. As a simple illustrative example, submitted
systems should accept the following sentence as input:

"Put down the green pyramid."

and produce the following RCL statement as output:

(event:
    (action: drop)
    (entity:
        (color: green)
        (type: prism)))


However, in contrast to the simple example above, the treebank is challenging to parse because it contains
linguistically-complex sentences that include:

  * Compositional syntax
  * Multiword spatial expressions
  * Anaphoric references
  * Ellipsis


A more complicated sentence illustrating this would be:

"Pick up the turquoise pyramid standing over a white cube and place [it] on top of the blue and green tower."

To make this parsing task more tractable, data and tools are provided to train supervised parsers, as well as
providing access to spatial context. Finally, for further incentive, it should be noted that the wider consequence
of participating in this task is to help work towards an interesting integrated solution that lies at the interface
of computational linguistics and robotics research. By combining a parser with the existing provided spatial planner,
participants will have completed an end-to-end working system that is able to control a robotic arm within a simulated
3D environment and that can be instructed using lingustically-rich natural language commands.

REFERENCES

Kais Dukes (2013). Semantic Annotation of Robotic Spatial Commands. Language and Technology Conference (LTC). Poznan,
Poland. http://www.kaisdukes.com/papers/spatial-ltc2013.pdf



***********************************************************************************************************************


Evaluation

INTRODUCTION

SemEval-2014 Task 6 uses data from The Robot Commands Treebank. The version of the treebank used for this task
contains a total of 3,409 sentences, each with a corresponding RCL representation and word-alignment data. The use
of word-alignment data is optional for this task, and is additional supervisory data to be used at the discretion
of task participants.

DATA SPLIT

For this task, the trial/training/test split of data sourced from the treebank is as follows:

  * Trial data - First 500 sentences in the treebank.
  * Training data - First 2,500 sentences in the treebank (trial data with an additional 2,000 sentences).
  * Evaluation data - The remaining 909 sentences positioned at the end of the treebank.


EVALUATION DATA

The complete dataset for the treebank is available for download here (all 3,409 sentences):
semevaltask6_evaldata.zip

Task participants are encouraged to use the Java API (available here) to access this data. For integrated systems,
the process for using the API to access the spatial planner during evaluation is the same process as during training.

EVALUATION INSTRUCTIONS

During evaluation, participants should perform the following steps:



1. Evaluation must be performed on a previously trained system. Only training data should be "visible" to the system
during the training period (i.e. only the previously published first 2,500 sentences in the treebank). Participants
are expected to stop developing/training their system before evaluation. The remaining 909 sentences in the
treebank must not be used or made visible during training.

2. To perform evaluation for each of the 909 evaluation sentences, trained systems should be given the sentence as
input, and produce an RCL representation.

3. Because the use of the API and spatial planner is not mandatory, and to encourage a low barrier to entry for
task participation, the evaluation metric used will be a simple direct comparison. For each evaluation sentence,
system output will be judged as correct if it exactly matches the expected RCL, and incorrect otherwise. Using this
strict metric, a total percentage accuracy score can be computed.

4. Task participants are required to submit two percentage scores, for the accuracy of their system with and without
using the spatial planner. Participants who are not using the spatial planner should mention this in their
submission, but should only submit a single accuracy score. Systems not using the spatial planner will not be
penalized for submitting a single score. Additionally, participants are asked to indicate whether or not
word-alignment data is used (a breakdown of accuracy with/without word-alignment data is not required in this case,
but may be interesting to note in your final paper).

5. There is no automated evaluation script for this task. Participants are required to compute accuracy scores as
part of their submission, using a simple direct match metric.

SUBMISSION

Task participants should have received submission instructions directly from the SemEval-2014 organizers. If you
have not received submission instructions, please get in touch.

REFERENCES

Because this task uses data from The Robot Commands Treebank, the following two references may be useful to note
as part of your submission. [1] provides a description of RCL and [2] contains a description of the data collection
process used to develop the treebank.

[1] Kais Dukes (2013a). Semantic Annotation of Robotic Spatial Commands. Language and Technology Conference (LTC).
Poznan, Poland.
http://www.kaisdukes.com/papers/spatial-ltc2013.pdf

[2] Kais Dukes (2013b). Train Robots: A Dataset for Natural Language Human-Robot Spatial Interaction through
Verbal Commands. International Conference on Social Robotics (ICSR). Embodied Communication of Goals and Intentions
Workshop. Bristol.
http://www.kaisdukes.com/papers/spatial-icsr2013.pdf


***********************************************************************************************************************


Results

TASK SUMMARY

This page lists submitted results for SemEval-2014 Task 6: Supervised Semantic Parsing of Robotic Spatial Commands.
As described in the evaluation section, this task uses data from The Robot Commands Treebank. Percentage accuracy
scores are shown below, with and without integrated spatial planning for additional situational context.

SUBMITTED RESULTS


Team and Affiliation 	Method (Statistical or rule-based) 	Results (parsing with integrated spatial planning) 	Results (parsing without integrated spatial planning)
Packard (University of Washington) 	Hybrid (rule-based English resource grammar, with statistical Berkeley parser for back-off) 	92.50 	90.50
Jung and Stoyanchev (AT&T Labs Research) 	Statistical (statistical maximum entropy parsing) 	87.35 	60.84
Evang and Bos (University of Groningen) 	Statistical (combinatory categorial grammar with structured perceptron) 	86.80 	79.21
Ljungl√∂f (University of Gothenburg) 	Rule-based (hand-crafted grammar) 	86.10 	51.50
Mattelaer, Verbeke and Nitti (Katholieke Universiteit Leuven) 	Statistical (combinatory categorial grammar parsing) 	71.29 	57.76
Kate (University of Wisconsin-Milwaukee) 	Statistical (Krisp parser) 	- 	45.98